# -*- coding: utf-8 -*-
"""Etransformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C14BEsRBq-SyEhuf-84YXsbItmFK4KRx
"""

import tensorflow as tf

class DTransformer(tf.keras.layers.Layer):
    """Encoder block: Pre-Norm MHA + FFN with residuals."""

    def __init__(
        self,
        embed_dim: int,
        num_heads: int = 12,
        expansion: int = 4,
        dropout_rate: float = 0.1,
        rich: int = 0,
        **kwargs,
    ):
        super().__init__(**kwargs)

        if (embed_dim + rich) % num_heads != 0:
            raise ValueError(
                "`(embed_dim + rich)` must be divisible by `num_heads`."
            )

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.expansion = expansion
        self.dropout_rate = dropout_rate
        self.rich = rich

        # ── layers ──────────────────────────────────────────────
        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.attn  = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=(embed_dim + rich) // num_heads,
        )
        self.drop1 = tf.keras.layers.Dropout(dropout_rate)

        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.ffn = tf.keras.Sequential(
            [
                tf.keras.layers.Dense(embed_dim * expansion, activation="gelu"),
                tf.keras.layers.Dense(embed_dim),
            ]
        )
        self.drop2 = tf.keras.layers.Dropout(dropout_rate)

    # -----------------------------------------------------------
    def call(self, x, mask=None, training=None):
        if x.shape[-1] != self.embed_dim:
            raise ValueError(
                f"Expected last dim {self.embed_dim}, got {x.shape[-1]}"
            )

        # 1. self-attention
        y = self.norm1(x)
        y = self.attn(y, y, attention_mask=mask, training=training)
        y = self.drop1(y, training=training)
        x = x + y  # residual

        # 2. feed-forward
        y = self.norm2(x)
        y = self.ffn(y)
        y = self.drop2(y, training=training)
        x = x + y  # residual

        return x

