# -*- coding: utf-8 -*-
"""SaadaBPE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DinuQwfrLacGiGa51UOPYgeOn_n4_1Mi
"""

import collections
import os
import json
import unicodedata
import re

class SaadaBPE:
    def __init__(self, space_symbol="▁", keep_diacritics=False):
        self.space_symbol = space_symbol
        self.keep_diacritics = keep_diacritics
        self.vocab = None
        self.merge_rules = []
        self.text = ""
        self.token2id = {}
        self.id2token = {}
        self.special_tokens = []

    def __call__(self, text, return_ids=False):
        """Shortcut for encoding text using the tokenizer object."""
        return self.encode(text, return_ids=return_ids)

    def __len__(self):
        return len(self.token2id)

    def read_text(self, filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            self.text = f.read()

    def preprocess_arabic(self, text):
        if not self.keep_diacritics:
            text = re.sub(r'[\u064B-\u0652]', '', text)  # Remove diacritics
        text = re.sub(r'[إأآا]', 'ا', text)            # Normalize alef
        text = re.sub(r'ى', 'ي', text)                # Normalize alef maqsura
        text = re.sub(r'ؤ', 'و', text)                # Normalize waw-hamza
        text = re.sub(r'ئ', 'ي', text)                # Normalize ya-hamza
        text = re.sub(r'ة', 'ه', text)                # Normalize taa marbuta
        text = re.sub(r'ك', 'ک', text)                # Normalize kaf to Persian kaf if desired
        return text

    def split_to_word_chars(self):
        lines = self.text.strip().split('\n')
        all_words = []
        for line in lines:
            line = self.preprocess_arabic(line)
            word_tuples = [
                tuple((self.space_symbol + word).strip())
                for word in line.strip().split()
                if word
            ]
            all_words.extend(word_tuples)
        self.vocab = collections.Counter(all_words)

    def get_pair_stats(self):
        pair_freq = collections.defaultdict(int)
        for word, freq in self.vocab.items():
            for pair in zip(word, word[1:]):
                pair_freq[pair] += freq
        return pair_freq

    def merge_pair(self, pair):
        bigram = ''.join(pair)
        new_vocab = {}
        for word, freq in self.vocab.items():
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(bigram)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_vocab[tuple(new_word)] = freq
        self.vocab = new_vocab
        self.merge_rules.append(pair)

    def train(self, num_merges=100, min_frequency=2, special_tokens=None):
        if special_tokens is None:
            special_tokens = []
        self.special_tokens = special_tokens

        token_set = set()
        for word in self.vocab:
            token_set.update(word)
        for line in self.text.strip().split('\n'):
            line = self.preprocess_arabic(line)
            for word in line.strip().split():
                token_set.update(self.space_symbol + word)

        sorted_tokens = sorted(token_set)
        self.token2id = {}
        self.id2token = {}

        for i, token in enumerate(special_tokens):
            self.token2id[token] = i
            self.id2token[i] = token

        next_id = len(special_tokens)
        for token in sorted_tokens:
            if token not in self.token2id:
                self.token2id[token] = next_id
                self.id2token[next_id] = token
                next_id += 1

        for _ in range(num_merges):
            pair_freq = self.get_pair_stats()
            if not pair_freq:
                break
            most_frequent_pair, freq = max(pair_freq.items(), key=lambda x: x[1])
            if freq < min_frequency:
                break
            self.merge_pair(most_frequent_pair)
            merged_token = ''.join(most_frequent_pair)
            if merged_token not in self.token2id:
                self.token2id[merged_token] = next_id
                self.id2token[next_id] = merged_token
                next_id += 1

    def get_vocab(self):
        return self.token2id

    def token_to_id(self, token):
        if token in self.token2id:
            return self.token2id[token]
        elif "<unk>" in self.token2id:
            return self.token2id["<unk>"]
        else:
            raise ValueError(f"Unknown token '{token}' and <unk> not defined.")

    def id_to_token(self, idx):
        return self.id2token.get(idx, "<unk>")

    def convert_tokens_to_ids(self, tokens):
        return [self.token_to_id(tok) for tok in tokens]

    def convert_ids_to_tokens(self, ids):
        return [self.id_to_token(i) for i in ids]

    def encode(self, text: str, return_ids=False):
        text = unicodedata.normalize("NFKC", text)
        text = self.preprocess_arabic(text)
        words = text.strip().split()
        output_tokens = []
        for word in words:
            tokens = list(self.space_symbol + word)
            i = 0
            while i < len(self.merge_rules):
                a, b = self.merge_rules[i]
                new_tokens = []
                j = 0
                while j < len(tokens):
                    if j < len(tokens) - 1 and tokens[j] == a and tokens[j + 1] == b:
                        new_tokens.append(a + b)
                        j += 2
                    else:
                        new_tokens.append(tokens[j])
                        j += 1
                if tokens == new_tokens:
                    i += 1
                else:
                    tokens = new_tokens
                    i = 0
            output_tokens.extend(tokens)
        if return_ids:
            return output_tokens, self.convert_tokens_to_ids(output_tokens)
        else:
            return output_tokens

    def decode(self, tokens: list[str]) -> str:
        words = []
        current_word = ""
        for token in tokens:
            if token.startswith(self.space_symbol):
                if current_word:
                    words.append(current_word)
                current_word = token.lstrip(self.space_symbol)
            else:
                current_word += token
        if current_word:
            words.append(current_word)
        return " ".join(words)

    def decode_from_ids(self, ids: list[int]) -> str:
        tokens = self.convert_ids_to_tokens(ids)
        return self.decode(tokens)

    def save_model(self, output_dir="tokenizer_output"):
        os.makedirs(output_dir, exist_ok=True)
        with open(os.path.join(output_dir, "merges.txt"), "w", encoding="utf-8") as f:
            for pair in self.merge_rules:
                f.write(f"{pair[0]} {pair[1]}\n")
        with open(os.path.join(output_dir, "vocab.txt"), "w", encoding="utf-8") as f:
            for token, idx in sorted(self.token2id.items(), key=lambda x: x[1]):
                f.write(f"{token}\t{idx}\n")
        with open(os.path.join(output_dir, "config.json"), "w", encoding="utf-8") as f:
            json.dump({
                "space_symbol": self.space_symbol,
                "special_tokens": self.special_tokens,
                "keep_diacritics": self.keep_diacritics
            }, f, ensure_ascii=False, indent=2)
        with open(os.path.join(output_dir, "tokenizer.json"), "w", encoding="utf-8") as f:
            tokenizer_json = {
                "model": {
                    "type": "BPE",
                    "vocab": self.token2id,
                    "merges": ["{} {}".format(a, b) for (a, b) in self.merge_rules]
                },
                "special_tokens": self.special_tokens,
                "normalizer": {"type": "NFKC"},
                "space_symbol": self.space_symbol,
                "keep_diacritics": self.keep_diacritics
            }
            json.dump(tokenizer_json, f, ensure_ascii=False, indent=2)

    def load_model(self, output_dir="tokenizer_output"):
        self.merge_rules = []
        with open(os.path.join(output_dir, "merges.txt"), "r", encoding="utf-8") as f:
            for line in f:
                a, b = line.strip().split()
                self.merge_rules.append((a, b))
        self.token2id = {}
        with open(os.path.join(output_dir, "vocab.txt"), "r", encoding="utf-8") as f:
            for line in f:
                token, idx = line.strip().split()
                self.token2id[token] = int(idx)
        self.id2token = {i: tok for tok, i in self.token2id.items()}
        config_path = os.path.join(output_dir, "config.json")
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
                self.space_symbol = config.get("space_symbol", self.space_symbol)
                self.special_tokens = config.get("special_tokens", [])
                self.keep_diacritics = config.get("keep_diacritics", False)

    def print_vocab(self):
        for token, idx in self.token2id.items():
            print(f"{idx}: {token}")