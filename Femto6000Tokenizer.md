# Femto6000Tokenizer

**Femto6000Tokenizer** is a Python tokenizer module that wraps a trained [SentencePiece](https://github.com/google/sentencepiece) model for easy and efficient encoding and decoding of Arabic text using subword tokenization. It's built as part of the `FemtoGpt` project to enable quick access to tokenizer functionality with minimal setup.

---

## âœ¨ Features

* ğŸ§  Built on [SentencePiece](https://github.com/google/sentencepiece)
* ğŸ”¡ Encodes text into subword tokens or IDs
* ğŸ” Decodes back to text
* ğŸ§© Supports vocab lookups: `id_to_piece`, `piece_to_id`
* ğŸ—‚ï¸ Loads tokenizer from local `.model` and `.vocab` files included with the package

---

## ğŸ“¦ Requirements

Install required packages:

```bash
pip install sentencepiece
```

If you're using TensorFlow as part of your GPT model training:

```bash
pip install tensorflow>=2.11
```

Or install everything together via GitHub:

```bash
pip install git+https://github.com/MohamedAliSaada/FemtoGpt.git
```

---

## ğŸ“ File Structure

Your directory should look like this:

```
FemtoGpt/
â”œâ”€â”€ FemtoGpt/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sp_tokenizer.py         # Contains Femto6000Tokenizer class
â”‚   â”œâ”€â”€ AR6000SPT.model         # SentencePiece model file
â”‚   â””â”€â”€ AR6000SPT.vocab         # Vocabulary file
â”œâ”€â”€ setup.py
â”œâ”€â”€ MANIFEST.in
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Setup for Local Development

### 1. Clone the Repo

```bash
git clone https://github.com/MohamedAliSaada/FemtoGpt.git
cd FemtoGpt
```

### 2. Install Dependencies

```bash
pip install -e .
```

This will install the package locally, along with its dependencies.

---

## ğŸš€ How to Use

### ğŸ In Python

```python
from FemtoGpt import Femto6000Tokenizer

tokenizer = Femto6000Tokenizer()

text = "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ ÙÙŠÙ…ØªÙˆ Ø¬ÙŠ Ø¨ÙŠ ØªÙŠ!"

ids = tokenizer.encode(text, out_type='id')
print("Token IDs:", ids)

tokens = tokenizer.encode(text, out_type='piece')
print("Subwords:", tokens)

decoded = tokenizer.decode(ids)
print("Decoded Text:", decoded)
```

### ğŸ“˜ In Google Colab

```python
!pip install git+https://github.com/MohamedAliSaada/FemtoGpt.git

from FemtoGpt import Femto6000Tokenizer

tokenizer = Femto6000Tokenizer()
print(tokenizer.encode("Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ù…ÙŠØ²", out_type='id'))
```

---

## ğŸ“œ MANIFEST & Packaging Notes

Ensure your `MANIFEST.in` includes the model files:

```
include FemtoGpt/AR6000SPT.model
include FemtoGpt/AR6000SPT.vocab
```

Also make sure `setup.py` contains:

```python
include_package_data=True
```

This ensures the tokenizer model files are bundled when the package is installed.

---

## âš ï¸ Troubleshooting

* `FileNotFoundError`: Make sure `.model` and `.vocab` files exist in the `FemtoGpt/` directory.
* In Colab, ensure files are in the current working directory if not using pip install.

---


